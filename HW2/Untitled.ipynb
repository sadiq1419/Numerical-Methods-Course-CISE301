{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d0a05d",
   "metadata": {},
   "source": [
    "# a must for the course\n",
    " * learn python \\[ basic upto methods, list, nested lists and module imports\\]\n",
    "     * be aware and leaen of numpy and matplot module VERY IMPORTANT\n",
    " * Learn about jupyter notebook VERY USEFUL AND HELPFUL\n",
    "      * learn about markdown and latex in jupyter really good for report writing\n",
    "\n",
    "#### This is important to know for all the hw\n",
    "#### Google collab is alternative to Jupyter. it is the same as Jupyter in the background except it uses google drive to store the files and needs online connection\n",
    "#### You can use MatLab if you want insteade of python and jupyter.\n",
    "\n",
    "#### Personally i prefer python with Jupyter as there is a lot of online help for it and videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4ebc1",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray;\" ></hr>\n",
    "<hr style=\"border:2px solid gray;\" ></hr>\n",
    "<hr style=\"border:2px solid gray;\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed755b",
   "metadata": {},
   "source": [
    "# steps of HW2:\n",
    "\n",
    "* write imports here some recommended imports.\n",
    "\n",
    "<code>\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import math\n",
    "</code>\n",
    "\n",
    "* use np.mod(your_ID,3) to know reminder after division by 3 to know you function f\n",
    "* use numpy to generate the cord of the reigon matplot(plt) and plt.conture to draw the function contours.\n",
    "\n",
    "\n",
    "* write a def for :\n",
    "    $f$,$df$,$\\frac{df}{dx}$, $\\frac{df}{dy}$,$\\frac{d^2f}{dx^2}$, $\\frac{d^2f}{dxdy}$, $\\frac{d^2f}{dydx}$, $\\frac{d^2f}{dy^2}$   \n",
    "* write a def for Jacobian at X as jacobian(X) = \\[$\\frac{df}{dx}$, $\\frac{df}{dy}$\\]\n",
    "* write a def for Hessian at X as Hessian(X) = $\\begin{bmatrix} \\frac{d^2f}{dx^2} & \\frac{d^2f}{dxdy}\\\\ \\frac{d^2f}{dydx} & \\frac{d^2f}{dy^2}\\end{bmatrix}$\n",
    "\n",
    "\n",
    "* note: X here is the pair (x,y) taken as input in f.\n",
    "\n",
    "## Algorithms:\n",
    "\n",
    "### before writing an algorithm:\n",
    "\n",
    "   * write a code that generate a list sample points in the region given\n",
    "    * np.random.uniform is very helpful function here\\\n",
    "    \n",
    "### Note:\n",
    "    for the stopping condition use math.dist(x1,x2) to get the euclidean distance also numpy has a function for it as well\n",
    "    or you can you |f(x1)-f(x2)| as a condition as well\n",
    " \n",
    "### Random Search: \n",
    "    \n",
    "   * Take the first sample point from the list as X_min\n",
    "   * Start a for loop over the length of the sample point list -1\n",
    "   * At every iteration of the loop check the current point X_min against X_next:\n",
    "   * if function at X_next is less than function at X_i then take X_i as x_min. else skip to the next sample point\n",
    "        * also check the stopping condition if X_i is taken if true break from the loop\n",
    "   * continue till you loop through all the sample points\n",
    "   * Note: save the initial sample point and all the samples that are taken in a list to plot them later\n",
    "        \n",
    "### Gradient Descent: \n",
    "* Take the input sample point\n",
    "* added to the list of point for plotting later on\n",
    "* start the loop\n",
    "* calculate the Gradient at the point. here the Jacobian is the Gradient of $f$\n",
    "* make an new Point cord as the following:\n",
    "    since the gradient is the Jacobian that equals \\[$\\frac{df}{dx}$, $\\frac{df}{dy}$\\]:\n",
    "    * $x = x+ alpha*grad[0]$\n",
    "    * $y = y+ alpha*grad[1]$\n",
    "        * NOTE: in Gradient descent a factor alpha is used in the update step to make the convergence less steep. This make sure the new points do not rush of away and divergence since the gradient can grow really quick.\n",
    "        \n",
    "* test the new point with the current point, if the condition is meet exist the loop\n",
    "* if not, then add the new point to the point list\n",
    "* Set the new point as the current point and loop over.\n",
    "\n",
    "    \n",
    "### Newton Method: \n",
    "* Take the input sample point\n",
    "* added to the list of point for plotting later on\n",
    "* start the loop\n",
    "* calculate the Gradient at the point. here the Jacobian is the Gradient of $f$ and calculate the Hessian at the point.\n",
    "* calculate the Hessian inverse (IH) this is just a normal 2x2 matrix inverse.\n",
    "\n",
    "* make an new Point cord as the following:\n",
    "    with the Gradient/Jacobian that equals \\[$\\frac{df}{dx}$, $\\frac{df}{dy}$\\]\n",
    "    and IH a 2x2 matrix:\n",
    "    * $x = x - IH[0][0]*jac[0] + IH[0][1]*jac[1])$\n",
    "    * $y = y - IH[1][0]*jac[0] + IH[1][1]*jac[1])$\n",
    "* test the new point with the current point, if the condition is meet exist the loop\n",
    "* if not, then add the new point to the point list\n",
    "* Set the new point as the current point and loop over.\n",
    "\n",
    "## Plotting the points:\n",
    "   To plot the point from every algorithm:\n",
    "   * First draw the contour map of f but do not show the plot.\n",
    "   * Use plt.scatter(x,y) where x is the list of x cords of the points that you saved when running the algorithm\n",
    "     and y x is the list of y cords of those point.\n",
    "    * plt.scatter will plot the points marker only with no connections\n",
    "    * Use plt.plot(x,y) to plot the line connecting the point\n",
    "    * Also you plot the min value on its own with plt.plot\n",
    "    * Note: you can change many attribute such as color, stroke-size and marker-shape in plt if you want.\n",
    "    \n",
    "## Last Part:\n",
    "   * Use markdown cells in jupyter to write your notes/report\n",
    "   * Structure the report however you like\n",
    "   * IMPORTANT explaine your code for the doctor. write comment in the code cells or write markdown but most IMPORTANT                   is to explaine the code.\n",
    "\n",
    "## Submiting (ask the doctor in what format to submit):\n",
    "   * in Jupyter you can download the notebook as PDF\n",
    "        * If download as pdf fails then download as HTML and then Ctrl+p and choose save as pdf as work around\n",
    "   * Also you can download the notebook it self and submit that as well\n",
    "       * the notebook format is .ipynb\n",
    "      \n",
    "# AND REMEMBER GOOGLE IS YOUR FRIEND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d669205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
